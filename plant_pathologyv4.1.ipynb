{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plant Pathology 2020 Kfold ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from random import seed\n",
    "from random import randint\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = '448x448_rgb_ResNet50'\n",
    "noise_typ='not'\n",
    "#SIZE = 56, 56\n",
    "#SIZE = 112, 112\n",
    "#SIZE = 224, 224\n",
    "SIZE = 448, 448\n",
    "gridsize = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"Data/images/\"\n",
    "TEST_PATH = \"Data/test.csv\"\n",
    "TRAIN_PATH = \"Data/train.csv\"\n",
    "SUBMISSION_PATH = \"Data/sample_submission.csv\"\n",
    "\n",
    "submission = pd.read_csv(SUBMISSION_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "train_data = pd.read_csv(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images: \",len(train_data))\n",
    "distrubution_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rush_train_data = train_data['image_id'][train_data['rust'] == 1]\n",
    "print('Number of images plants with rush: ',len(rush_train_data))\n",
    "distrubution_plot.append(len(rush_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scab_train_data = train_data['image_id'][train_data['scab'] == 1]\n",
    "print('Number of images plants with scab: ',len(scab_train_data))\n",
    "distrubution_plot.append(len(scab_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_diseases_train_data = train_data['image_id'][train_data['multiple_diseases'] == 1]\n",
    "print('Number of images plants with multiple diseases: ',len(multiple_diseases_train_data))\n",
    "distrubution_plot.append(len(multiple_diseases_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_train_data = train_data['image_id'][train_data['healthy'] == 1]\n",
    "print('Number of images healthy plants: ',len(healthy_train_data))\n",
    "distrubution_plot.append(len(healthy_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['rush', 'scab', 'multiple_diseases', 'healthy']\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.bar(names, distrubution_plot)\n",
    "plt.suptitle('distrubution plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add noise to images\n",
    "def noisy(noise_typ,image):\n",
    "    if noise_typ == \"not\":\n",
    "        return image\n",
    "    if noise_typ == \"gauss\":\n",
    "        row,col,ch= image.shape\n",
    "        mean = 0\n",
    "        var = 0.1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "        gauss = gauss.reshape(row,col,ch)\n",
    "        noisy = image + gauss\n",
    "        return noisy\n",
    "    elif noise_typ == \"s&p\":\n",
    "        row,col,ch = image.shape\n",
    "        s_vs_p = 0.5\n",
    "        amount = 0.004\n",
    "        out = np.copy(image)\n",
    "        # Salt mode\n",
    "        num_salt = np.ceil(amount * image.size * s_vs_p)\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "              for i in image.shape]\n",
    "        out[coords] = 1\n",
    "\n",
    "        # Pepper mode\n",
    "        num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "              for i in image.shape]\n",
    "        out[coords] = 0\n",
    "        return out\n",
    "    elif noise_typ == \"poisson\":\n",
    "        vals = len(np.unique(image))\n",
    "        vals = 2 ** np.ceil(np.log2(vals))\n",
    "        noisy = np.random.poisson(image * vals) / float(vals)\n",
    "        return noisy\n",
    "    elif noise_typ ==\"speckle\":\n",
    "        row,col,ch = image.shape\n",
    "        gauss = np.random.randn(row,col,ch)\n",
    "        gauss = gauss.reshape(row,col,ch)        \n",
    "        noisy = image + image * gauss\n",
    "    elif noise_typ ==\"clahe\":    \n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "        lab_planes = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(gridsize,gridsize))\n",
    "        lab_planes[0] = clahe.apply(lab_planes[0])\n",
    "        lab = cv2.merge(lab_planes)\n",
    "        noisy = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "        return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "train_shape = []\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    file_path = train_data[\"image_id\"][i] + \".jpg\"\n",
    "    image = cv2.imread(IMAGE_PATH + file_path)\n",
    "    image = cv2.resize(image, SIZE)\n",
    "    image = noisy(noise_typ,image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = np.asarray(image)\n",
    "    height, width, channels = image.shape\n",
    "    train_shape.append([height, width, channels])\n",
    "    image = Image.fromarray(image)\n",
    "    image = np.asarray(image)\n",
    "    train.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    file_path = test_data[\"image_id\"][i] + \".jpg\"\n",
    "    image = cv2.imread(IMAGE_PATH + file_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, SIZE)\n",
    "    image = noisy(noise_typ,image)\n",
    "    image = np.asarray(image)\n",
    "    height, width, channels = image.shape\n",
    "    test_images.append(image)  \n",
    "test_images = np.array(test_images)\n",
    "test_images = test_images.astype(\"float32\") / 255.0\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.imshow(cv2.resize(train[random.randint(1,100)], (205, 136)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.imshow(cv2.resize(train[random.randint(1,100)], (205, 136)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.imshow(cv2.resize(train[random.randint(1,100)], (205, 136)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.imshow(cv2.resize(train[random.randint(1,100)], (205, 136)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.imshow(cv2.resize(train[random.randint(1,100)], (205, 136)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.imshow(cv2.resize(train[random.randint(1,100)], (205, 136)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(cv2.resize(train[random.randint(1,100)], (205, 136)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "train = train.astype(\"float32\") / 255.0\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = train_data\n",
    "prepared_train_data = np.array(df_train_data)\n",
    "prepared_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_all = prepared_train_data[:, [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, prepared_train_data = shuffle(train, prepared_train_data, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_train = prepared_train_data[:, [0]]\n",
    "prepared_train_data = np.delete(prepared_train_data, 0, 1)\n",
    "df_prepared_train_data = pd.DataFrame(prepared_train_data)\n",
    "prepared_train_data = df_prepared_train_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import adam\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import History \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import adam\n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    plt.subplot(211)\n",
    "    plt.title( 'Cross Entropy Loss' )\n",
    "    plt.plot(history.history[ 'loss' ], color= 'blue' , label= 'train' )\n",
    "    plt.plot(history.history[ 'val_loss' ], color= 'orange' , label= 'test' )\n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title( 'Classification Accuracy')\n",
    "    plt.plot(history.history[ 'accuracy' ], color= 'blue' , label= 'train' )\n",
    "    plt.plot(history.history[ 'val_accuracy' ], color= 'orange' , label= 'test' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.00000001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "def define_model():\n",
    "# load model\n",
    "    model = ResNet50(weights='imagenet', include_top=False, input_shape=(SIZE[0], SIZE[1], 3))\n",
    "# mark loaded layers as not trainable\n",
    "    for layer in model.layers:\n",
    "        #layer.trainable = False\n",
    "        layer.trainable = True\n",
    "# add new classifier layers\n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    class1 = Dense(500, activation= 'relu' , kernel_initializer= 'he_uniform' )(flat1)\n",
    "    output = Dense(4, activation= 'softmax' )(class1)\n",
    "# define new model\n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "# compile model\n",
    "    #opt = SGD(lr=0.001, momentum=0.9)\n",
    "    opt=adam(lr=0.00000001)\n",
    "    model.compile(optimizer=opt, loss= 'categorical_crossentropy' , metrics=[ 'accuracy' ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-storage\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"Project-XXXXXXXXXX.json\"\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, destination_blob_name\n",
    "        )\n",
    "    )\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\n",
    "        \"Blob {} downloaded to {}.\".format(\n",
    "            source_blob_name, destination_file_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_train_data = np.argmax(prepared_train_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 5\n",
    "skf = StratifiedKFold(n_splits=kfold,shuffle=False)\n",
    "for m, (train_index, test_index) in enumerate(skf.split(train, prepared_train_data, key_train)):\n",
    "    print('Fold %d/%d' % (m + 1, kfold))\n",
    "    temp_i = str(m + 1)\n",
    "    x_train, x_test = train[train_index], train[test_index]\n",
    "    y_train, y_test = prepared_train_data[train_index], prepared_train_data[test_index]\n",
    "    key_train_val, key_test_val = key_train[train_index], key_train[test_index]\n",
    "    \n",
    "    y_train = keras.utils.to_categorical(y_train, 4)\n",
    "    y_test = keras.utils.to_categorical(y_test, 4)\n",
    "    y_all = keras.utils.to_categorical(prepared_train_data, 4)\n",
    "    \n",
    "    # define model \n",
    "    model = define_model()\n",
    "    # create data generator\n",
    "    print(model.summary())\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range=0,  # Randomly zoom image\n",
    "        width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "    )  \n",
    "\n",
    "    datagen.fit(x_train)\n",
    "    \n",
    "    # simple early stopping\n",
    "    history = History()\n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "    \n",
    "    callbacks_list = [lrate, es, history]\n",
    "\n",
    "    \n",
    "\n",
    "    historylist=[[],[],[],[],[],[],[]]\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), validation_data = (x_test,y_test),epochs = 5,verbose = 1, callbacks=callbacks_list)\n",
    "    listofkeys = [k for k in history.history.keys()]\n",
    "    history.history.keys()\n",
    "    for i in range(len(listofkeys)):\n",
    "        for j in range(len(history.history[listofkeys[i]])):\n",
    "            historylist[i].append(history.history[listofkeys[i]][j])\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), validation_data = (x_test,y_test),epochs = 70,verbose = 1, callbacks=callbacks_list)\n",
    "    for i in range(len(listofkeys)):\n",
    "        for j in range(len(history.history[listofkeys[i]])):\n",
    "            historylist[i].append(history.history[listofkeys[i]][j])\n",
    "\n",
    "    _, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "    print( 'Train > %.3f' % (acc * 100.0))\n",
    "    _, acc = model.evaluate(x_test,y_test, verbose=1)\n",
    "    print( 'Test > %.3f' % (acc * 100.0))\n",
    "        \n",
    "  \n",
    "\n",
    "    #submission\n",
    "    submission_predict = model.predict(test_images, verbose=1)\n",
    "    submission = pd.read_csv(SUBMISSION_PATH)\n",
    "    submission.loc[:, 'healthy':] = submission_predict\n",
    "    name_of_sub=  'sub/sub/' + MODEL_NAME + '_sub_' + temp_i + '.csv'\n",
    "    submission.to_csv(name_of_sub, index=False)\n",
    "    print(name_of_sub)\n",
    "\n",
    "    #save model\n",
    "    name_of_model=  'models/pretrained/' + MODEL_NAME + '_sub_' + temp_i + '.h5'\n",
    "    print(temp_i,\" \",name_of_model)\n",
    "    model.save(name_of_model)\n",
    "    print(\"Saved model to disk\")\n",
    "    summarize_diagnostics(history)\n",
    "    tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
